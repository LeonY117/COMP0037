{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "5edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Question 3\n",
    "\n",
    "Task: finding locations to clean while avoiding high traffic areas\n",
    "\n",
    "Environment: screenshot here\n",
    "\n",
    "Action space: {rotate CW, rotate CCW, move forward} \n",
    "\n",
    "    - if the robot is currently facing up, and it wants to do a 180 turn, \n",
    "    - it'll need to 1) rotate CW, 2) rotate CW, 3) move foward\n",
    "\n",
    "\n",
    "Reward function: \n",
    "\n",
    "    * +10 for reaching cleaning locations\n",
    "    * -10 for entering high traffic areas\n",
    "    * -1 for any action\n",
    "\n",
    "**Folder setup**\n",
    "```\n",
    "Question3\n",
    "- question3.ipynb\n",
    "- clean_airport_env.py\n",
    "- generalized_policy_iteration.py\n",
    "- plotting.py\n",
    "- robot_enums.py\n",
    "- simple_grid_env.py"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 720x720 with 0 Axes>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Import the functions provided to us\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from simple_grid_env import SimpleGridEnv\n",
    "from clean_airport_env import CleanAirportEnv\n",
    "from robot_enums import Heading\n",
    "from robot_enums import Action\n",
    "from plotting import ValueFunctionPlotter\n",
    "from plotting import PolicyPlotter"
   ]
  },
  {
   "source": [
    "## Policy Iteration Pseudo code (General)\n",
    "\n",
    "**1.Initialization **\n",
    "\n",
    "$ V(s)\\in\\mathbb{R}\\text{ and }\\pi(s)\\in\\mathcal{A}(s)\\text{ arbitrarily for all } s\\in\\mathcal{S}$\n",
    "\n",
    "**2.Policy Evaluation **\n",
    "\n",
    "Repeat\n",
    "    \n",
    "$\\quad \\Delta \\leftarrow 0$\n",
    "\n",
    "$\\quad \\text{For each } s\\in\\mathcal{S}:$\n",
    "\n",
    "$\\qquad v \\leftarrow V(s)$\n",
    "\n",
    "$\\qquad V(s) \\leftarrow \\sum_{s', r} p(s'|s, \\pi(s))[r+\\gamma V'(s)]$\n",
    "\n",
    "$\\qquad \\Delta \\leftarrow \\max(\\Delta, |v-V(s)|)$\n",
    "\n",
    "$\\text{until } \\Delta < \\theta (10^{-9})$\n",
    "\n",
    "**3.Policy Improvement**\n",
    "\n",
    "$stablePolicy \\leftarrow True$\n",
    "\n",
    "For each $s\\in\\mathcal{S}:$\n",
    "\n",
    "$\\quad oldAction \\leftarrow \\pi(s)$\n",
    "\n",
    "$\\quad \\pi(s) \\leftarrow \\text{argmax}_a\\sum_{s', r}p(s'|s, a)[r+\\gamma V(s')]$\n",
    "\n",
    "$\\quad \\text{If } oldAction \\neq \\pi(s), \\text{then } stablePolicy \\leftarrow false$\n",
    "\n",
    "$\\text{If } stablePolicy, \\text{then stop and return }V\\approx v_* \\text{ and }\\pi \\approx \\pi_*; \\text{else go to 2}$\n",
    "    "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Policy Iteration Pseudo code (Deterministic)\n",
    "\n",
    "**1.Initialization **\n",
    "\n",
    "$ V(s)\\in\\mathbb{R}\\text{ and }\\pi(s)\\in\\mathcal{A}(s)\\text{ arbitrarily for all } s\\in\\mathcal{S}$\n",
    "\n",
    "**2.Policy Evaluation **\n",
    "\n",
    "Repeat\n",
    "    \n",
    "$\\quad \\Delta \\leftarrow 0$\n",
    "\n",
    "$\\quad \\text{For each } s\\in\\mathcal{S}:$\n",
    "\n",
    "$\\qquad v \\leftarrow V(s)$\n",
    "\n",
    "$\\qquad V(s) \\leftarrow [r+\\gamma V'(s)]$\n",
    "\n",
    "$\\qquad \\Delta \\leftarrow \\max(\\Delta, |v-V(s)|)$\n",
    "\n",
    "$\\text{until } \\Delta < \\theta (10^{-9})$\n",
    "\n",
    "**3.Policy Improvement**\n",
    "\n",
    "$stablePolicy \\leftarrow True$\n",
    "\n",
    "For each $s\\in\\mathcal{S}:$\n",
    "\n",
    "$\\quad oldAction \\leftarrow \\pi(s)$\n",
    "\n",
    "$\\quad \\pi(s) \\leftarrow \\text{argmax}_a[r+\\gamma V(s')]$\n",
    "\n",
    "$\\quad \\text{If } oldAction \\neq \\pi(s), \\text{then } stablePolicy \\leftarrow false$\n",
    "\n",
    "$\\text{If } stablePolicy, \\text{then stop and return }V\\approx v_* \\text{ and }\\pi \\approx \\pi_*; \\text{else go to 2}$\n",
    "    "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Value Iteration Pseudo Code (General)\n",
    "\n",
    "$ \\text{Initiate array }V(s) \\text{ to be 0 for all } s\\in\\mathcal{S}$\n",
    "\n",
    "Repeat\n",
    "\n",
    "$ \\quad v \\leftarrow V(s)$\n",
    "\n",
    "$ \\quad V(s) \\leftarrow \\max_a \\sum_{s', r} p(s', r|s, a)[r+\\gamma V(s')]$\n",
    "\n",
    "$ \\quad \\Delta \\leftarrow \\max (\\Delta, | v-V(s)|)$\n",
    "\n",
    "until $\\Delta < \\theta$ (a small positive number)\n",
    "\n",
    "Ouput a deterministic policy $\\pi \\approx \\pi_*$, such that \n",
    "\n",
    "$\\quad \\pi(s)=\\text{argmax}_a \\sum_{s', r} p(s', r|s, a)[r+\\gamma V(s')]$\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Value Iteration Pseudo Code (Deterministic)\n",
    "\n",
    "$ \\text{Initiate array }V(s) \\text{ to be 0 for all } s\\in\\mathcal{S}$\n",
    "\n",
    "Repeat\n",
    "\n",
    "$ \\quad v \\leftarrow V(s)$\n",
    "\n",
    "$ \\quad V(s) \\leftarrow \\max_a [r+\\gamma V(s')]$\n",
    "\n",
    "$ \\quad \\Delta \\leftarrow \\max (\\Delta, | v-V(s)|)$\n",
    "\n",
    "until $\\Delta < \\theta$ (a small positive number)\n",
    "\n",
    "Ouput a deterministic policy $\\pi \\approx \\pi_*$, such that \n",
    "\n",
    "$\\quad \\pi(s)=\\text{argmax}_a [r+\\gamma V(s')]$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralPolicyIteration(object):\n",
    "\n",
    "    def __init__(self, environment, discount_factor):\n",
    "        ''' Constructs a new policy iteration class instance\n",
    "        Args:\n",
    "        Environment: a simpleGrid() object \n",
    "            contains the grid space, the win states, the lose states, and functions to compute dynamic updates\n",
    "        Discount_factor: float, set to 1.0\n",
    "        '''\n",
    "        self.env = environment\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "    def get_state_value(self, state, V, action):\n",
    "        ''' Returns the state value of the given state\n",
    "        Args\n",
    "        ----------------------------------------\n",
    "        state: tuple\n",
    "            in form of (i, j, k) representing the coordinate of the cell\n",
    "        V: np.array(), with shape (row, col, headings)\n",
    "            represents the value grid\n",
    "        action: enum\n",
    "            the particular action that will be taken from this state\n",
    "\n",
    "        Returns\n",
    "        ----------------------------------------\n",
    "        reward: float\n",
    "            value of this state\n",
    "        '''\n",
    "        # Retrieves the next state, and the reward using the environment functions\n",
    "        next_state, reward, terminal = self.env.dynamics(state, action)\n",
    "        # Note that there was a bug in the original code in simple_grid_env.py, where the terminal state outputs were (next_state, terminal, reward) instead of (next_state, reward, terminal). This code assumes that this bug is fixed. \n",
    "        if self.env.is_terminal(state):\n",
    "            # only the reward needs to be returned as there is no next state\n",
    "            return reward\n",
    "        else:\n",
    "            # For all other cases, return the reward + discounted next state reward (as in psuedo code)\n",
    "            return reward + self.discount_factor * V[tuple(next_state)]\n",
    "\n",
    "    def policy_evaluation(self, policy, theta=1e-9, max_iterations=1e3):\n",
    "        '''\n",
    "        Evaluates a given policy\n",
    "\n",
    "        Args\n",
    "        ----------------------------------------\n",
    "        policy: np.array(), with shape (row, col, headings)\n",
    "            the policy to be evaluated\n",
    "        theta: float\n",
    "            for convergence\n",
    "        max_iterations: int \n",
    "            default to 1e3, used to exit recursion\n",
    "\n",
    "        Returns\n",
    "        ----------------------------------------\n",
    "        V: np.array(), with shape (row, col, headings)\n",
    "            A value grid representing the value of every state\n",
    "        '''\n",
    "        # Initialise value function for each state as zero\n",
    "        V = np.zeros([self.env.grid_rows, self.env.grid_cols, 4])\n",
    "        delta = 0\n",
    "        for _ in range(int(max_iterations)):\n",
    "            for h in Heading:\n",
    "                for i in range(self.env.grid_rows):\n",
    "                    for j in range(self.env.grid_cols):\n",
    "                        s = (i, j, h)\n",
    "                        v = V[s]\n",
    "                        V[s] = self.get_state_value(s, V, policy[s])\n",
    "                        delta = max(delta, abs(v-V[s]))\n",
    "            if delta < theta:\n",
    "                return V\n",
    "        return V\n",
    "\n",
    "    def policy_improvement(self, policy, V):\n",
    "\n",
    "        ''' Iterates the entire policy grid once by picking a new policy which maximises the state value for every state\n",
    "        Args\n",
    "        ----------------------------------------\n",
    "        policy: np.array(), with shape (row, col, headings)\n",
    "            the policy to be updated\n",
    "        V: np.array(), with shape (row, col, headings)\n",
    "            A value grid representing the value of every state\n",
    "\n",
    "        Returns\n",
    "        ----------------------------------------\n",
    "        policy: np.array(), with shape (row, col, headings)\n",
    "            updated policy grid\n",
    "        stable_policy: bool\n",
    "            whether or not the policy stable\n",
    "        '''\n",
    "\n",
    "        stable_policy = True\n",
    "        for h in Heading:\n",
    "            for i in range(self.env.grid_rows):\n",
    "                for j in range(self.env.grid_cols):\n",
    "                    s = (i, j, h)\n",
    "                    old_action = policy[s]\n",
    "                    policy[s] = np.argmax([self.get_state_value(s, V, a) \n",
    "                                            for a in list(Action)[:-1]])\n",
    "                    if old_action != policy[s]:\n",
    "                        stable_policy=False\n",
    "        return policy, stable_policy\n",
    "\n",
    "\n",
    "    def policy_iteration(self, max_iterations=1e3):\n",
    "\n",
    "        '''Performs policy iteration and returns the converged policy and value grids.\n",
    "\n",
    "        Args\n",
    "        ----------------------------------------\n",
    "        max_iteration: int, default to 1000, \n",
    "            Maximum number of iterations before automatically returned.\n",
    "\n",
    "        Returns\n",
    "        ----------------------------------------\n",
    "        policy: np.array(), with shape (row, col, headings)\n",
    "            final, converged policy grid\n",
    "        V: np.array(), with shape (row, col, headings)\n",
    "            final, converged value grid\n",
    "        '''\n",
    "        # Initialize a random policy\n",
    "        policy = np.random.randint(0, self.env.nA, size=(self.env.grid_rows, self.env.grid_cols, len(Heading)))\n",
    "        V = np.zeros([self.env.grid_rows, self.env.grid_cols, len(Heading)])\n",
    "\n",
    "        # Create functions for plotting\n",
    "        value_function_plotter = ValueFunctionPlotter()\n",
    "        policy_plotter = PolicyPlotter()\n",
    "\n",
    "        for i in range(int(max_iterations)):      \n",
    "            V = self.policy_evaluation(policy)\n",
    "            policy, stable = self.policy_improvement(policy, V)\n",
    "            if stable:\n",
    "                # Plot converged value and policy graphs \n",
    "                # value_function_plotter.plot(V)\n",
    "                # policy_plotter.plot(policy)   \n",
    "                return policy, V\n",
    "\n",
    "    def value_iteration(self, theta=1e-9, max_iterations=1e3):\n",
    "        '''Performs value iteration and returns the the optimum policy and value grids\n",
    "        Args\n",
    "        ----------------------------------------\n",
    "        theta: float, default to 1e-9, \n",
    "            used for convergence\n",
    "        max_iteration: int, default to 1000, \n",
    "            Maximum number of iterations before automatically returned.\n",
    "\n",
    "        Returns\n",
    "        ----------------------------------------\n",
    "        policy: np.array(), with shape (row, col, headings)\n",
    "            final, converged policy grid\n",
    "        V: np.array(), with shape (row, col, headings)\n",
    "            final, converged value grid\n",
    "        '''\n",
    "        # Initialize a random policy\n",
    "        policy = np.random.randint(0, self.env.nA, size=(self.env.grid_rows, self.env.grid_cols, len(Heading)))\n",
    "        V = np.zeros([self.env.grid_rows, self.env.grid_cols, len(Heading)])\n",
    "\n",
    "        delta = 0\n",
    "        # Converge state value to optimum value\n",
    "        for _ in range(int(max_iterations)):\n",
    "            for h in Heading:\n",
    "                for i in range(self.env.grid_rows):\n",
    "                    for j in range(self.env.grid_cols):\n",
    "                        s = (i, j, h)\n",
    "                        v = V[s]\n",
    "                        best_a = np.argmax([self.get_state_value(s, V, a) for a in list(Action)[:-1]])\n",
    "                        V[s] = self.get_state_value(s, V, best_a)\n",
    "                        delta = max(delta, abs(v-V[s]))\n",
    "            if delta < theta:\n",
    "                break\n",
    "        \n",
    "        # Get policy which maximizes state value for every state\n",
    "        for h in Heading:\n",
    "            for i in range(self.env.grid_rows):\n",
    "                for j in range(self.env.grid_cols):\n",
    "                    s = (i, j, h)\n",
    "                    policy[s] = np.argmax([self.get_state_value(s, V, a) for a in list(Action)[:-1]])\n",
    "        \n",
    "        # Create functions for plotting\n",
    "        value_function_plotter = ValueFunctionPlotter()\n",
    "        policy_plotter = PolicyPlotter()\n",
    "\n",
    "        # Plot converged value and policy graphs \n",
    "        # value_function_plotter.plot(V)\n",
    "        # policy_plotter.plot(policy)\n",
    "\n",
    "        return policy, V\n",
    "\n",
    "    def validate(self, policy, V):\n",
    "        V_validate = self.policy_evaluation(policy)\n",
    "        for h in Heading:\n",
    "            for i in range(self.env.grid_rows):\n",
    "                for j in range(self.env.grid_cols):\n",
    "                    s = (i, j, h)\n",
    "                    if V[s] != V_validate[s]:\n",
    "                        return False\n",
    "        return True\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_environment(use_simple_grid=True):\n",
    "    if use_simple_grid:\n",
    "        # Simple Grid Env\n",
    "        grid_rows = 3\n",
    "        grid_cols = 4\n",
    "        start_state = (0, 0, Heading.NORTH)\n",
    "\n",
    "        # Simple example\n",
    "        win_states = [(0, 3)]\n",
    "        lose_states = [(1, 3)]\n",
    "\n",
    "        # Simple gridworld\n",
    "        robot = SimpleGridEnv(grid_rows, grid_cols, start_state, win_states, lose_states)\n",
    "\n",
    "    else:\n",
    "        \n",
    "        # Creating an object of type CleanAirportEnv\n",
    "        # Locations matching the grid airport\n",
    "        grid_rows = 7\n",
    "        grid_cols = 9\n",
    "        start_state = (0, 0, Heading.EAST)\n",
    "        cleaning_locations = [(0, 5), (1, 4), (1, 5), (2, 0), (2, 1), (2, 2), (4, 0), (4, 7)]\n",
    "        traffic_locations = [(1, 2), (5, 1), (5, 3), (5, 5), (5, 7)]\n",
    "        trapdoor_location = [(2, 8, Heading.EAST), (4, 8, Heading.EAST)]\n",
    "        customs_barrier_row = 3\n",
    "        # Create object with init method requirements\n",
    "        robot = CleanAirportEnv(grid_rows, grid_cols, start_state, cleaning_locations, traffic_locations,trapdoor_location, customs_barrier_row)\n",
    "    return robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot = get_environment(use_simple_grid = True)\n",
    "\n",
    "# Run the requested algorithms\n",
    "gpi = GeneralPolicyIteration(robot, discount_factor=1.0)\n",
    "\n",
    "# policy, value = gpi.policy_iteration()\n",
    "policy, value = gpi.value_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# Validate\n",
    "gpi.validate(policy, value)"
   ]
  },
  {
   "source": [
    "### Calculate time required"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Policy iteration: 6.78426\n",
      "Value iteration: 3.38841\n"
     ]
    }
   ],
   "source": [
    "# Simple gridworld\n",
    "robot = get_environment(use_simple_grid = True)\n",
    "\n",
    "# Run the requested algorithms\n",
    "gpi = GeneralPolicyIteration(robot, discount_factor=1.0)\n",
    "\n",
    "total_time = 0\n",
    "for _ in range(3):\n",
    "    tic = time.perf_counter()\n",
    "    policy, value = gpi.policy_iteration()\n",
    "    toc = time.perf_counter()\n",
    "    total_time += (toc-tic)\n",
    "print(f'Policy iteration: {total_time/3:.6}')\n",
    "\n",
    "total_time = 0\n",
    "for _ in range(3):\n",
    "    tic = time.perf_counter()\n",
    "    policy, value = gpi.value_iteration()\n",
    "    toc = time.perf_counter()\n",
    "    total_time += (toc-tic)\n",
    "print(f'Value iteration: {total_time/3:.6}')"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}