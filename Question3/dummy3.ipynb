{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "5edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Question 3\n",
    "\n",
    "Task: finding locations to clean while avoiding high traffic areas\n",
    "\n",
    "Environment: screenshot here\n",
    "\n",
    "Action space: {rotate CW, rotate CCW, move forward} \n",
    "\n",
    "    - if the robot is currently facing up, and it wants to do a 180 turn, \n",
    "    - it'll need to 1) rotate CW, 2) rotate CW, 3) move foward\n",
    "\n",
    "\n",
    "Reward function: \n",
    "\n",
    "    * +10 for reaching cleaning locations\n",
    "    * -10 for entering high traffic areas\n",
    "    * -1 for any action\n",
    "\n",
    "**Folder setup**\n",
    "```\n",
    "Question3\n",
    "- question3.ipynb\n",
    "- clean_airport_env.py\n",
    "- generalized_policy_iteration.py\n",
    "- plotting.py\n",
    "- robot_enums.py\n",
    "- simple_grid_env.py\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 720x720 with 0 Axes>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Import the functions provided to us\n",
    "import numpy as np\n",
    "from simple_grid_env import SimpleGridEnv\n",
    "from clean_airport_env import CleanAirportEnv\n",
    "from robot_enums import Heading\n",
    "from robot_enums import Action\n",
    "from plotting import ValueFunctionPlotter\n",
    "from plotting import PolicyPlotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward_mapper  = {0: (1, 0),\n",
    "#                    1: (0, 1),\n",
    "#                    2: (-1, 0),\n",
    "#                    3: (0, -1)}\n",
    "\n",
    "forward_mapper  = {0: (1, 0),\n",
    "                   1: (0, 1),\n",
    "                   2: (-1, 0),\n",
    "                   3: (0, -1)}                  \n",
    "\n",
    "direction_lookup = ['N', 'E', 'S', 'W']\n",
    "action_lookup = ['Foward', 'CW', 'CCW', 'terminate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'policy' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9d8dbb33d215>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'policy' is not defined"
     ]
    }
   ],
   "source": [
    "print(policy[:, :, 1])\n",
    "\n",
    "policy[0, 3, 1]"
   ]
  },
  {
   "source": [
    "**General Policy Iteration Class**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralPolicyIteration(object):\n",
    "\n",
    "    def __init__(self, environment, discount_factor):\n",
    "        ''' Constructs a new policy iteration class instance\n",
    "        Args:\n",
    "        Environment: a simpleGrid() object \n",
    "            contains the grid space, the win states, and the lose states\n",
    "        Discount_factor: float\n",
    "        '''\n",
    "        self.env = environment\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "    def get_state_value(self, i, j, k, policy, iteration):\n",
    "        '''\n",
    "        gets the value for the current state \n",
    "        \n",
    "        Args\n",
    "        ----------------------------------------\n",
    "        i: row \n",
    "        j: col\n",
    "        k: direction the robot is facing\n",
    "        policy: the current policy (can be improved by not passing the entire policy)\n",
    "        iteration: iteration count, exits recursion if larger than max_iteration \n",
    "        \n",
    "        Returns\n",
    "        ----------------------------------------\n",
    "        state value\n",
    "        '''\n",
    "        max_iterations = 1000\n",
    "\n",
    "        if iteration >= max_iterations:\n",
    "            return 0\n",
    "\n",
    "        if (i, j) in win_states:\n",
    "            return 10\n",
    "        elif (i, j) in lose_states:\n",
    "            return -10\n",
    "\n",
    "        action = policy[i][j][k]\n",
    "        \n",
    "        # Compute the next state ii, jj, kk\n",
    "        ii, jj, kk = i, j, k\n",
    "        if action == 0: # Forward\n",
    "            ii, jj = forward_mapper[k][0] + i, forward_mapper[k][1] + j\n",
    "        elif action == 1: # CW\n",
    "            kk = (k+1) % 4\n",
    "        elif action == 2: # CCW\n",
    "            kk = (k-1) % 4\n",
    "\n",
    "        # Bound to grid\n",
    "        ii = max(ii, 0)\n",
    "        ii = min(ii, 2)\n",
    "        jj = max(jj, 0)\n",
    "        jj = min(jj, 3)\n",
    "\n",
    "        return self.get_state_value(ii, jj, kk, policy, iteration+1) - 1\n",
    "\n",
    "    def policy_evaluation(self, policy, theta=1e-9, max_iterations=1e3):\n",
    "        '''\n",
    "        Evaluates a given policy\n",
    "\n",
    "        Args\n",
    "        ----------------------------------------\n",
    "        policy: the policy to be evaluated row x col x 4 array\n",
    "        theta: float\n",
    "        max_iterations: int, default to 1e3, used to exit recursion\n",
    "\n",
    "        Returns\n",
    "        ----------------------------------------\n",
    "        V: row x col x 4 array indicating value of every state\n",
    "        '''\n",
    "        # Initialise value function for each state as zero, V(terminal) = 0, others can be arbitrary\n",
    "        V = np.zeros([self.env.grid_rows, self.env.grid_cols, 4])\n",
    "\n",
    "        for k in range(4):\n",
    "            for i, row in enumerate(V[:, :, k]):\n",
    "                for j, s in enumerate(V[i, :, k]):\n",
    "                    # print(V_north[i][j])\n",
    "                    V[i, j, k] = self.get_state_value(i, j, k, policy, 0)\n",
    "        \n",
    "        return V\n",
    "\n",
    "    def policy_improvement(self, policy, V):\n",
    "\n",
    "        '''\n",
    "        Iterates the entire policy grid once by picking a new policy which maximises the state value for every state\n",
    "        Args\n",
    "        ----------------------------------------\n",
    "        policy: the policy grid to be updated\n",
    "        V: the current state value function to be compared to\n",
    "\n",
    "        Returns\n",
    "        ----------------------------------------\n",
    "        policy: updated policy grid\n",
    "        stable_policy: bool, whether or not the policy stable\n",
    "        '''\n",
    "\n",
    "        stable_policy = True\n",
    "\n",
    "        for k in range(4):\n",
    "            for i, row in reversed(list(enumerate(V[:, :, k]))):\n",
    "                for j, state in reversed(list(enumerate(V[i, :, k]))):\n",
    "                    dummy_policy = policy.copy()\n",
    "                    for action in range(3):\n",
    "                        old_value = V[i, j, k]\n",
    "                        dummy_policy[i, j, k] = action\n",
    "                        new_value = self.get_state_value(i, j, k, dummy_policy, 0)\n",
    "                        if new_value > old_value:\n",
    "                            old_value = new_value\n",
    "                            policy[i, j, k] = action\n",
    "                            stable_policy=False\n",
    "        \n",
    "        return policy, stable_policy\n",
    "\n",
    "\n",
    "    def policy_iteration(self, max_iterations=1e3):\n",
    "\n",
    "        '''\n",
    "        Policy iteration (using iterative policy evaluation).\n",
    "\n",
    "        Args\n",
    "        ----------------------------------------\n",
    "        max_iteration: Maximum number of iterations before automatically returned.\n",
    "\n",
    "        Returns\n",
    "        ----------------------------------------\n",
    "        policy: updated policy grid\n",
    "        V: value grid\n",
    "        '''\n",
    "        # Initialize a random policy\n",
    "        # np.random.seed(123)\n",
    "        policy = np.random.randint(0, self.env.nA, size=(self.env.grid_rows, self.env.grid_cols, len(Heading)))\n",
    "        V = np.zeros([self.env.grid_rows, self.env.grid_cols, len(Heading)])\n",
    "\n",
    "        # Create functions for plotting\n",
    "        value_function_plotter = ValueFunctionPlotter()\n",
    "        # value_function_plotter.plot(V)\n",
    "        policy_plotter = PolicyPlotter()\n",
    "        # policy_plotter.plot(policy)\n",
    "\n",
    "        # for i in range(int(max_iterations)):\n",
    "        for i in range(10):\n",
    "            policy, stable = self.policy_improvement(policy, V)\n",
    "            \n",
    "            value_function_plotter.plot(V)\n",
    "            if stable:\n",
    "                print('is stable')\n",
    "                policy_plotter.plot(policy)        \n",
    "                V = self.policy_evaluation(policy)      \n",
    "                break\n",
    "            \n",
    "\n",
    "        return policy, V\n",
    "\n",
    "    \"\"\"\n",
    "    @info:  Value iteration\n",
    "            theta: Stopping condition.\n",
    "            max_iteration: Maximum number of iterations before automatically returned.\n",
    "    @return: policy: State to action mapping.\n",
    "             V: State-value function.\n",
    "    \"\"\"\n",
    "\n",
    "    def value_iteration(self, theta=1e-9, max_iterations=1e3):\n",
    "        # Initialize a random policy\n",
    "        policy = np.random.randint(0, self.env.nA, size=(self.env.grid_rows, self.env.grid_cols, len(Heading)))\n",
    "        V = np.zeros([self.env.grid_rows, self.env.grid_cols, len(Heading)])\n",
    "\n",
    "        # Create functions for plotting\n",
    "        value_function_plotter = ValueFunctionPlotter()\n",
    "        policy_plotter = PolicyPlotter()\n",
    "\n",
    "        value_function_plotter.plot(V)\n",
    "        policy_plotter.plot(policy)\n",
    "\n",
    "        print(\"value_iteration: Insert your implementation here\")\n",
    "\n",
    "        return policy, V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-6-faafae9cf510>, line 22)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-faafae9cf510>\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    trapdoor_location, customs_barrier_row)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Simple Grid Env\n",
    "grid_rows = 3\n",
    "grid_cols = 4\n",
    "start_state = (0, 0, Heading.NORTH)\n",
    "\n",
    "# Simple example\n",
    "win_states = [(0, 3)]\n",
    "lose_states = [(1, 3)]\n",
    "\n",
    "# Simple gridworld\n",
    "robot = SimpleGridEnv(grid_rows, grid_cols, start_state, win_states, lose_states)\n",
    "?or_location, customs_barrier_row)\n",
    "# Run the requested algorithms\n",
    "gpi = GeneralPolicyIteration(robot, discount_factor=1.0)\n",
    "\n",
    "policy, value = gpi.policy_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}